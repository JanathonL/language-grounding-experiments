{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting environment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile environment.py\n",
    "import numpy as np\n",
    "import random\n",
    "def k_hot(n, lst):\n",
    "    k = len(lst)\n",
    "    assert k > 0\n",
    "    ret = np.zeros(k * n)\n",
    "    ret[[i * n + x for i, x in enumerate(lst)]] = 1\n",
    "    return ret\n",
    "\n",
    "class Game(object):\n",
    "    def __init__(self, n, k):\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.target = None\n",
    "        self.states = []\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = np.random.choice(range(self.n), self.k, replace=False).tolist()\n",
    "        self.target = 0\n",
    "    \n",
    "    def sender_input(self):\n",
    "        return k_hot(self.n, self.states)\n",
    "        \n",
    "    def receiver_input(self, val):\n",
    "        shuffled_index ,shuffled_states = zip(*sorted(zip(range(self.k), self.states), key=lambda _: random.random()))\n",
    "        self.target = list(shuffled_index).index(0)\n",
    "        return np.concatenate((k_hot(self.n, shuffled_states), [val]))\n",
    "    \n",
    "    def reward(self, out):\n",
    "        assert self.target is not None\n",
    "        if out == self.target:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    g = Game(10, 3)\n",
    "    print(g.sender_input())\n",
    "    print(g.receiver_input(0.5))\n",
    "    g.reset()\n",
    "    print(g.sender_input())\n",
    "    print(g.receiver_input(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "# About discriministic policy gradient\n",
    "# Please refer to the following papers: \n",
    "# 1. http://proceedings.mlr.press/v32/silver14.pdf\n",
    "# 2. https://arxiv.org/pdf/1509.02971.pdf\n",
    "\n",
    "from environment import Game\n",
    "from model import *\n",
    "from itertools import count\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "bound = [0., 1.]\n",
    "\n",
    "n_numbers = 50\n",
    "n_k = 2\n",
    "# Hyper parameters\n",
    "explore_sigma = 0.4 / (n_numbers + 1)\n",
    "n_r = 20\n",
    "\n",
    "def explore_noise():\n",
    "    return np.random.normal(0, explore_sigma)\n",
    "\n",
    "def toggle_module(mod, requires_grad=True):\n",
    "    for p in mod.parameters():\n",
    "        p.requires_grad = requires_grad\n",
    "\n",
    "def update_module(mod_name):\n",
    "    global optim_r, optim_sa, optim_sc, game_pool, R, SA, SC\n",
    "    \n",
    "    s_input = []\n",
    "    for i in range(n_games):\n",
    "        s_input.append(game_pool[i].sender_input())\n",
    "\n",
    "    if mod_name == 'actor':\n",
    "        s_input = Variable(torch.Tensor(s_input))\n",
    "        action_s = SA(s_input)\n",
    "        \n",
    "        r_input = []\n",
    "        for i in range(n_games):\n",
    "            r_input.append(game_pool[i].receiver_input(action_s.data[i][0] + explore_noise()))\n",
    "\n",
    "        r_input = Variable(torch.Tensor(r_input), volatile=True)\n",
    "        toggle_module(R, requires_grad=False)\n",
    "        action_r = R(r_input)\n",
    "        discrete_action_r = torch.max(action_r, 1)[1]\n",
    "        reward = []\n",
    "        for i in range(n_games):\n",
    "            reward.append(game_pool[i].reward(discrete_action_r.data[i]))\n",
    "        \n",
    "        r_input.volatile=False\n",
    "        r_input.requires_grad=True\n",
    "        predict_reward = SC(r_input)\n",
    "        target_reward = torch.FloatTensor(reward).view(-1, 1)\n",
    "        #optim_sc.zero_grad()\n",
    "        loss = F.mse_loss(predict_reward, Variable(target_reward))\n",
    "        loss.backward()\n",
    "        #optim_sc.step()\n",
    "        \n",
    "        optim_sa.zero_grad()\n",
    "        action_s.backward(r_input.grad[:, -1].contiguous().view(-1, 1))\n",
    "        optim_sa.step()\n",
    "    elif mod_name == 'critic':\n",
    "        s_input = Variable(torch.Tensor(s_input), volatile=True)\n",
    "        action_s = SA(s_input)\n",
    "\n",
    "        r_input = []\n",
    "        for i in range(n_games):\n",
    "            r_input.append(game_pool[i].receiver_input(action_s.data[i][0] + explore_noise()))\n",
    "\n",
    "        r_input = Variable(torch.Tensor(r_input), requires_grad=False)\n",
    "        \n",
    "        toggle_module(R)\n",
    "        action_r = R(r_input)\n",
    "        discrete_action_r = torch.max(action_r, 1)[1]\n",
    "        reward = []\n",
    "        labels = []\n",
    "        for i in range(n_games):\n",
    "            reward.append(game_pool[i].reward(discrete_action_r.data[i]))\n",
    "            labels.append(game_pool[i].target)\n",
    "\n",
    "        predict_reward = SC(r_input)\n",
    "        target_reward = torch.FloatTensor(reward).view(-1, 1)\n",
    "        optim_sc.zero_grad()\n",
    "        loss = F.mse_loss(predict_reward, Variable(target_reward))\n",
    "        loss.backward()\n",
    "        optim_sc.step()\n",
    "        \n",
    "        optim_r.zero_grad()\n",
    "        loss = F.nll_loss(action_r, Variable(torch.LongTensor(labels)))\n",
    "        loss.backward()\n",
    "        optim_r.step()\n",
    "    else:\n",
    "        raise ValueError('Invalid parameter')\n",
    "    \n",
    "    for i in range(n_games):\n",
    "    game_pool[i].reset()\n",
    "    \n",
    "    return sum(reward) / 32.\n",
    "\n",
    "n_hidden = 200\n",
    "n_games = 32\n",
    "game_pool = []\n",
    "for i in range(n_games):\n",
    "    game_pool.append(Game(n_numbers, n_k))\n",
    "\n",
    "R = Receiver(n_numbers, n_k, n_hidden)\n",
    "SA = SenderActor(n_numbers, n_k, n_hidden)\n",
    "SC = SenderCritic(n_numbers, n_k, n_hidden)\n",
    "\n",
    "optim_r = optim.Adam(R.parameters(), lr=1e-3)\n",
    "optim_sa = optim.Adam(SA.parameters(), lr=1e-3)\n",
    "optim_sc = optim.Adam(SC.parameters(), lr=1e-3)\n",
    "\n",
    "running_succ_rate = 0.5\n",
    "for epoch in count(1):\n",
    "    if epoch % n_r == 0:\n",
    "        succ_rate = update_module('actor')\n",
    "    else:\n",
    "        succ_rate = update_module('critic')\n",
    "    \n",
    "    running_succ_rate = running_succ_rate * 0.95 + succ_rate * 0.05\n",
    "    print('Epoch {}: successful_rate = {}'.format(epoch, running_succ_rate))\n",
    "    if running_succ_rate > 0.9:\n",
    "        break\n",
    "\n",
    "stat = [[] for _ in range(n_numbers)]\n",
    "for _ in range(100):\n",
    "    for i in range(n_games):\n",
    "        game_pool[i].reset()\n",
    "    \n",
    "    s_input = []\n",
    "    for i in range(n_games):\n",
    "        s_input.append(game_pool[i].sender_input())\n",
    "    \n",
    "    s_input = Variable(torch.Tensor(s_input), volatile=True)\n",
    "    action_s = SA(s_input).view(-1).data.tolist()\n",
    "    for i in range(n_games):\n",
    "        select_val = game_pool[i].x if game_pool[i].target == 0 else game_pool[i].y\n",
    "        stat[select_val].append(action_s[i])\n",
    "\n",
    "for j in range(n_numbers):\n",
    "    print np.mean(stat[j]), np.std(stat[j])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "colors = cm.rainbow(np.linspace(0, 1, n_numbers))\n",
    "for x, c in enumerate(colors):\n",
    "    for y in stat[x]:\n",
    "        plt.scatter(x, y, color=c)\n",
    "plt.savefig('viz.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "Format:\n",
    "input vector: (kn + 1)\n",
    "output vector(prob): (k)\n",
    "\"\"\"\n",
    "class Receiver(nn.Module):\n",
    "    def __init__(self, n, k, hid):\n",
    "        super(Receiver, self).__init__()\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.hid = hid\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(k * n + 1, hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid, hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid, k),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_size = input.size(1)\n",
    "        assert input_size == self.n * self.k + 1\n",
    "        return self.net(input)\n",
    "\n",
    "\"\"\"\n",
    "Format:\n",
    "input vector: (kn)\n",
    "output vector(real number range from 0 to 1): (1)\n",
    "\"\"\"\n",
    "class SenderActor(nn.Module):\n",
    "    def __init__(self, n, k, hid):\n",
    "        super(SenderActor, self).__init__()\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.hid = hid\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(k * n, hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid, hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_size = input.size(1)\n",
    "        assert input_size == self.n * self.k\n",
    "        return self.net(input)\n",
    "\n",
    "\"\"\"\n",
    "Format:\n",
    "input vector: (kn + 1)\n",
    "output vector(Q value): (1)\n",
    "\"\"\"\n",
    "class SenderCritic(nn.Module):\n",
    "    def __init__(self, n, k, hid):\n",
    "        super(SenderCritic, self).__init__()\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.hid = hid\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(k * n + 1, hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid, hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_size = input.size(1)\n",
    "        assert input_size == self.n * self.k + 1\n",
    "        return self.net(input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
